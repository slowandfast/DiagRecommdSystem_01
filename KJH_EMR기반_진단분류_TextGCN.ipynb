{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkld5p5KJ2mk0AoG2xIyIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowandfast/DiagRecommdSystem_01/blob/main/KJH_EMR%EA%B8%B0%EB%B0%98_%EC%A7%84%EB%8B%A8%EB%B6%84%EB%A5%98_TextGCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===============================\n",
        "1. remove_words.py"
      ],
      "metadata": {
        "id": "3oRaN5h9gvko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/yao8839836/text_gcn.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6W51sZSnX2f",
        "outputId": "17bc1ba1-0175-4a09-c76c-51c41accdf7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text_gcn'...\n",
            "remote: Enumerating objects: 26801, done.\u001b[K\n",
            "remote: Total 26801 (delta 0), reused 0 (delta 0), pack-reused 26801\u001b[K\n",
            "Receiving objects: 100% (26801/26801), 861.51 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "Updating files: 100% (26397/26397), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/text_gcn/')"
      ],
      "metadata": {
        "id": "FmrlR5PvsUwT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python remove_words.py mr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "e5jJiBVExbQd",
        "outputId": "b6b822a3-d155-4758-eded-8b659828aaea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-aab50f69ccc8>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-aab50f69ccc8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python remove_words.py mr\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEKhLva-xbBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "90h85STnX2B8",
        "outputId": "df045c9a-8589-4df4-b7d0-e9054fd8533f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Use: python remove_words.py <dataset>",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Use: python remove_words.py <dataset>\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from utils import clean_str, loadWord2Vec\n",
        "import sys\n",
        "\n",
        "if len(sys.argv) != 2:\n",
        "\tsys.exit(\"Use: python remove_words.py <dataset>\")\n",
        "\n",
        "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
        "dataset = sys.argv[1]\n",
        "\n",
        "if dataset not in datasets:\n",
        "\tsys.exit(\"wrong dataset name\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# Read Word Vectors\n",
        "# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n",
        "# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "# word_embeddings_dim = len(embd[0])\n",
        "# dataset = '20ng'\n",
        "\n",
        "doc_content_list = []\n",
        "f = open('/content/text_gcn/data/corpus/' + dataset + '.txt', 'rb')\n",
        "# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
        "for line in f.readlines():\n",
        "    doc_content_list.append(line.strip().decode('latin1'))\n",
        "f.close()\n",
        "\n",
        "\n",
        "word_freq = {}  # to remove rare words\n",
        "\n",
        "for doc_content in doc_content_list:\n",
        "    temp = clean_str(doc_content)\n",
        "    words = temp.split()\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "clean_docs = []\n",
        "for doc_content in doc_content_list:\n",
        "    temp = clean_str(doc_content)\n",
        "    words = temp.split()\n",
        "    doc_words = []\n",
        "    for word in words:\n",
        "        # word not in stop_words and word_freq[word] >= 5\n",
        "        if dataset == 'mr':\n",
        "            doc_words.append(word)\n",
        "        elif word not in stop_words and word_freq[word] >= 5:\n",
        "            doc_words.append(word)\n",
        "\n",
        "    doc_str = ' '.join(doc_words).strip()\n",
        "    #if doc_str == '':\n",
        "        #doc_str = temp\n",
        "    clean_docs.append(doc_str)\n",
        "\n",
        "clean_corpus_str = '\\n'.join(clean_docs)\n",
        "\n",
        "f = open('/content/text_gcn/data/corpus/' + dataset + '.clean.txt', 'w')\n",
        "#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\n",
        "f.write(clean_corpus_str)\n",
        "f.close()\n",
        "\n",
        "#dataset = '20ng'\n",
        "min_len = 10000\n",
        "aver_len = 0\n",
        "max_len = 0\n",
        "\n",
        "f = open('/content/text_gcn/data/corpus/' + dataset + '.clean.txt', 'r')\n",
        "#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    temp = line.split()\n",
        "    aver_len = aver_len + len(temp)\n",
        "    if len(temp) < min_len:\n",
        "        min_len = len(temp)\n",
        "    if len(temp) > max_len:\n",
        "        max_len = len(temp)\n",
        "f.close()\n",
        "aver_len = 1.0 * aver_len / len(lines)\n",
        "print('min_len : ' + str(min_len))\n",
        "print('max_len : ' + str(max_len))\n",
        "print('average_len : ' + str(aver_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===============================\n",
        "2. build_graph.py"
      ],
      "metadata": {
        "id": "p-ZJuRAEgiXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from utils import loadWord2Vec, clean_str\n",
        "from math import log\n",
        "from sklearn import svm\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sys\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "if len(sys.argv) != 2:\n",
        "\tsys.exit(\"Use: python build_graph.py <dataset>\")\n",
        "\n",
        "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
        "# build corpus\n",
        "dataset = sys.argv[1]\n",
        "\n",
        "if dataset not in datasets:\n",
        "\tsys.exit(\"wrong dataset name\")\n",
        "\n",
        "# Read Word Vectors\n",
        "# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
        "# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
        "#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "# word_embeddings_dim = len(embd[0])\n",
        "\n",
        "word_embeddings_dim = 300\n",
        "word_vector_map = {}\n",
        "\n",
        "# shulffing\n",
        "doc_name_list = []\n",
        "doc_train_list = []\n",
        "doc_test_list = []\n",
        "\n",
        "f = open('data/' + dataset + '.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    doc_name_list.append(line.strip())\n",
        "    temp = line.split(\"\\t\")\n",
        "    if temp[1].find('test') != -1:\n",
        "        doc_test_list.append(line.strip())\n",
        "    elif temp[1].find('train') != -1:\n",
        "        doc_train_list.append(line.strip())\n",
        "f.close()\n",
        "# print(doc_train_list)\n",
        "# print(doc_test_list)\n",
        "\n",
        "doc_content_list = []\n",
        "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    doc_content_list.append(line.strip())\n",
        "f.close()\n",
        "# print(doc_content_list)\n",
        "\n",
        "train_ids = []\n",
        "for train_name in doc_train_list:\n",
        "    train_id = doc_name_list.index(train_name)\n",
        "    train_ids.append(train_id)\n",
        "print(train_ids)\n",
        "random.shuffle(train_ids)\n",
        "\n",
        "# partial labeled data\n",
        "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
        "\n",
        "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
        "f = open('data/' + dataset + '.train.index', 'w')\n",
        "f.write(train_ids_str)\n",
        "f.close()\n",
        "\n",
        "test_ids = []\n",
        "for test_name in doc_test_list:\n",
        "    test_id = doc_name_list.index(test_name)\n",
        "    test_ids.append(test_id)\n",
        "print(test_ids)\n",
        "random.shuffle(test_ids)\n",
        "\n",
        "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
        "f = open('data/' + dataset + '.test.index', 'w')\n",
        "f.write(test_ids_str)\n",
        "f.close()\n",
        "\n",
        "ids = train_ids + test_ids\n",
        "print(ids)\n",
        "print(len(ids))\n",
        "\n",
        "shuffle_doc_name_list = []\n",
        "shuffle_doc_words_list = []\n",
        "for id in ids:\n",
        "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
        "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
        "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
        "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
        "\n",
        "f = open('data/' + dataset + '_shuffle.txt', 'w')\n",
        "f.write(shuffle_doc_name_str)\n",
        "f.close()\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
        "f.write(shuffle_doc_words_str)\n",
        "f.close()\n",
        "\n",
        "# build vocab\n",
        "word_freq = {}\n",
        "word_set = set()\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_set.add(word)\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab = list(word_set)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_doc_list = {}\n",
        "\n",
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    appeared = set()\n",
        "    for word in words:\n",
        "        if word in appeared:\n",
        "            continue\n",
        "        if word in word_doc_list:\n",
        "            doc_list = word_doc_list[word]\n",
        "            doc_list.append(i)\n",
        "            word_doc_list[word] = doc_list\n",
        "        else:\n",
        "            word_doc_list[word] = [i]\n",
        "        appeared.add(word)\n",
        "\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "word_id_map = {}\n",
        "for i in range(vocab_size):\n",
        "    word_id_map[vocab[i]] = i\n",
        "\n",
        "vocab_str = '\\n'.join(vocab)\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_vocab.txt', 'w')\n",
        "f.write(vocab_str)\n",
        "f.close()\n",
        "\n",
        "'''\n",
        "Word definitions begin\n",
        "'''\n",
        "'''\n",
        "definitions = []\n",
        "\n",
        "for word in vocab:\n",
        "    word = word.strip()\n",
        "    synsets = wn.synsets(clean_str(word))\n",
        "    word_defs = []\n",
        "    for synset in synsets:\n",
        "        syn_def = synset.definition()\n",
        "        word_defs.append(syn_def)\n",
        "    word_des = ' '.join(word_defs)\n",
        "    if word_des == '':\n",
        "        word_des = '<PAD>'\n",
        "    definitions.append(word_des)\n",
        "\n",
        "string = '\\n'.join(definitions)\n",
        "\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
        "f.write(string)\n",
        "f.close()\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
        "tfidf_matrix_array = tfidf_matrix.toarray()\n",
        "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n",
        "\n",
        "word_vectors = []\n",
        "\n",
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    vector = tfidf_matrix_array[i]\n",
        "    str_vector = []\n",
        "    for j in range(len(vector)):\n",
        "        str_vector.append(str(vector[j]))\n",
        "    temp = ' '.join(str_vector)\n",
        "    word_vector = word + ' ' + temp\n",
        "    word_vectors.append(word_vector)\n",
        "\n",
        "string = '\\n'.join(word_vectors)\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
        "f.write(string)\n",
        "f.close()\n",
        "\n",
        "word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
        "_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "word_embeddings_dim = len(embd[0])\n",
        "'''\n",
        "\n",
        "'''\n",
        "Word definitions end\n",
        "'''\n",
        "\n",
        "# label list\n",
        "label_set = set()\n",
        "for doc_meta in shuffle_doc_name_list:\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label_set.add(temp[2])\n",
        "label_list = list(label_set)\n",
        "\n",
        "label_list_str = '\\n'.join(label_list)\n",
        "f = open('data/corpus/' + dataset + '_labels.txt', 'w')\n",
        "f.write(label_list_str)\n",
        "f.close()\n",
        "\n",
        "# x: feature vectors of training docs, no initial features\n",
        "# slect 90% training set\n",
        "train_size = len(train_ids)\n",
        "val_size = int(0.1 * train_size)\n",
        "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
        "# different training rates\n",
        "\n",
        "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
        "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
        "\n",
        "f = open('data/' + dataset + '.real_train.name', 'w')\n",
        "f.write(real_train_doc_names_str)\n",
        "f.close()\n",
        "\n",
        "row_x = []\n",
        "col_x = []\n",
        "data_x = []\n",
        "for i in range(real_train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            # print(doc_vec)\n",
        "            # print(np.array(word_vector))\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_x.append(i)\n",
        "        col_x.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
        "\n",
        "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
        "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
        "    real_train_size, word_embeddings_dim))\n",
        "\n",
        "y = []\n",
        "for i in range(real_train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    y.append(one_hot)\n",
        "y = np.array(y)\n",
        "print(y)\n",
        "\n",
        "# tx: feature vectors of test docs, no initial features\n",
        "test_size = len(test_ids)\n",
        "\n",
        "row_tx = []\n",
        "col_tx = []\n",
        "data_tx = []\n",
        "for i in range(test_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i + train_size]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_tx.append(i)\n",
        "        col_tx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
        "\n",
        "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
        "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
        "                   shape=(test_size, word_embeddings_dim))\n",
        "\n",
        "ty = []\n",
        "for i in range(test_size):\n",
        "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ty.append(one_hot)\n",
        "ty = np.array(ty)\n",
        "print(ty)\n",
        "\n",
        "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
        "# (a superset of x)\n",
        "# unlabeled training instances -> words\n",
        "\n",
        "word_vectors = np.random.uniform(-0.01, 0.01,\n",
        "                                 (vocab_size, word_embeddings_dim))\n",
        "\n",
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    if word in word_vector_map:\n",
        "        vector = word_vector_map[word]\n",
        "        word_vectors[i] = vector\n",
        "\n",
        "row_allx = []\n",
        "col_allx = []\n",
        "data_allx = []\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i))\n",
        "        col_allx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
        "for i in range(vocab_size):\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i + train_size))\n",
        "        col_allx.append(j)\n",
        "        data_allx.append(word_vectors.item((i, j)))\n",
        "\n",
        "\n",
        "row_allx = np.array(row_allx)\n",
        "col_allx = np.array(col_allx)\n",
        "data_allx = np.array(data_allx)\n",
        "\n",
        "allx = sp.csr_matrix(\n",
        "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
        "\n",
        "ally = []\n",
        "for i in range(train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ally.append(one_hot)\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    ally.append(one_hot)\n",
        "\n",
        "ally = np.array(ally)\n",
        "\n",
        "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "\n",
        "'''\n",
        "Doc word heterogeneous graph\n",
        "'''\n",
        "\n",
        "# word co-occurence with context windows\n",
        "window_size = 20\n",
        "windows = []\n",
        "\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    length = len(words)\n",
        "    if length <= window_size:\n",
        "        windows.append(words)\n",
        "    else:\n",
        "        # print(length, length - window_size + 1)\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "            # print(window)\n",
        "\n",
        "\n",
        "word_window_freq = {}\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])\n",
        "\n",
        "word_pair_count = {}\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i]\n",
        "            word_i_id = word_id_map[word_i]\n",
        "            word_j = window[j]\n",
        "            word_j_id = word_id_map[word_j]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "\n",
        "row = []\n",
        "col = []\n",
        "weight = []\n",
        "\n",
        "# pmi as weights\n",
        "\n",
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    count = word_pair_count[key]\n",
        "    word_freq_i = word_window_freq[vocab[i]]\n",
        "    word_freq_j = word_window_freq[vocab[j]]\n",
        "    pmi = log((1.0 * count / num_window) /\n",
        "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "\n",
        "# word vector cosine similarity as weights\n",
        "\n",
        "'''\n",
        "for i in range(vocab_size):\n",
        "    for j in range(vocab_size):\n",
        "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
        "            vector_i = np.array(word_vector_map[vocab[i]])\n",
        "            vector_j = np.array(word_vector_map[vocab[j]])\n",
        "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
        "            if similarity > 0.9:\n",
        "                print(vocab[i], vocab[j], similarity)\n",
        "                row.append(train_size + i)\n",
        "                col.append(train_size + j)\n",
        "                weight.append(similarity)\n",
        "'''\n",
        "# doc word frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[doc_id]\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_size)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
        "                  word_doc_freq[vocab[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n",
        "node_size = train_size + vocab_size + test_size\n",
        "adj = sp.csr_matrix(\n",
        "    (weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# dump objects\n",
        "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
        "pkl.dump(x, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
        "pkl.dump(y, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.tx\".format(dataset), 'wb')\n",
        "pkl.dump(tx, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.ty\".format(dataset), 'wb')\n",
        "pkl.dump(ty, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.allx\".format(dataset), 'wb')\n",
        "pkl.dump(allx, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.ally\".format(dataset), 'wb')\n",
        "pkl.dump(ally, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
        "pkl.dump(adj, f)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "XC_QA_MFgh4k",
        "outputId": "a1066a6b-69ec-4522-a14c-ce3b0d9fffb4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '路' (U+00B7) (utils.py, line 108)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-12738845f962>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<cell line: 7>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from utils import loadWord2Vec, clean_str\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/utils.py\"\u001b[0;36m, line \u001b[0;32m108\u001b[0m\n\u001b[0;31m    <title>text_gcn/utils.py at master 路 yao8839836/text_gcn 路 GitHub</title>\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '路' (U+00B7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===============================\n",
        "3. train.py"
      ],
      "metadata": {
        "id": "APS1JVMPg4A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "from utils import *\n",
        "from models import GCN, MLP\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if len(sys.argv) != 2:\n",
        "\tsys.exit(\"Use: python train.py <dataset>\")\n",
        "\n",
        "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
        "dataset = sys.argv[1]\n",
        "\n",
        "if dataset not in datasets:\n",
        "\tsys.exit(\"wrong dataset name\")\n",
        "\n",
        "\n",
        "# Set random seed\n",
        "seed = random.randint(1, 200)\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "# 'cora', 'citeseer', 'pubmed'\n",
        "flags.DEFINE_string('dataset', dataset, 'Dataset string.')\n",
        "# 'gcn', 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
        "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 0,\n",
        "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
        "flags.DEFINE_integer('early_stopping', 10,\n",
        "                     'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "\n",
        "# Load data\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n",
        "    FLAGS.dataset)\n",
        "print(adj)\n",
        "# print(adj[0], adj[1])\n",
        "features = sp.identity(features.shape[0])  # featureless\n",
        "\n",
        "print(adj.shape)\n",
        "print(features.shape)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "if FLAGS.model == 'gcn':\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)]  # Not used\n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    # helper variable for sparse dropout\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
        "}\n",
        "\n",
        "# Create model\n",
        "print(features[2][1])\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "sess = tf.Session(config=session_conf)\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(\n",
        "        features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(\n",
        "        features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
        "                     model.layers[0].embedding], feed_dict=feed_dict)\n",
        "\n",
        "    # Validation\n",
        "    cost, acc, pred, labels, duration = evaluate(\n",
        "        features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(\n",
        "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Testing\n",
        "test_cost, test_acc, pred, labels, test_duration = evaluate(\n",
        "    features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
        "\n",
        "test_pred = []\n",
        "test_labels = []\n",
        "print(len(test_mask))\n",
        "for i in range(len(test_mask)):\n",
        "    if test_mask[i]:\n",
        "        test_pred.append(pred[i])\n",
        "        test_labels.append(labels[i])\n",
        "\n",
        "print(\"Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
        "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
        "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
        "\n",
        "# doc and word embeddings\n",
        "print('embeddings:')\n",
        "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\n",
        "train_doc_embeddings = outs[3][:train_size]  # include val docs\n",
        "test_doc_embeddings = outs[3][adj.shape[0] - test_size:]\n",
        "\n",
        "print(len(word_embeddings), len(train_doc_embeddings),\n",
        "      len(test_doc_embeddings))\n",
        "print(word_embeddings)\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_vocab.txt', 'r')\n",
        "words = f.readlines()\n",
        "f.close()\n",
        "\n",
        "vocab_size = len(words)\n",
        "word_vectors = []\n",
        "for i in range(vocab_size):\n",
        "    word = words[i].strip()\n",
        "    word_vector = word_embeddings[i]\n",
        "    word_vector_str = ' '.join([str(x) for x in word_vector])\n",
        "    word_vectors.append(word + ' ' + word_vector_str)\n",
        "\n",
        "word_embeddings_str = '\\n'.join(word_vectors)\n",
        "f = open('data/' + dataset + '_word_vectors.txt', 'w')\n",
        "f.write(word_embeddings_str)\n",
        "f.close()\n",
        "\n",
        "doc_vectors = []\n",
        "doc_id = 0\n",
        "for i in range(train_size):\n",
        "    doc_vector = train_doc_embeddings[i]\n",
        "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
        "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
        "    doc_id += 1\n",
        "\n",
        "for i in range(test_size):\n",
        "    doc_vector = test_doc_embeddings[i]\n",
        "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
        "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
        "    doc_id += 1\n",
        "\n",
        "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
        "f = open('data/' + dataset + '_doc_vectors.txt', 'w')\n",
        "f.write(doc_embeddings_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "-99YaeTEhAUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}